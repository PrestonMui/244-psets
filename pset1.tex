\documentclass[11pt]{article}
\usepackage{minibox}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#1 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\subsection*{Identification I (OLS)}

Prove that $\theta \equiv \{ \beta, F_X(\cdot), F_{u|X}(\cdot) \}$ is point identified in OLS:

First, $F_X(\cdot)$ is point identified from the joint distribution of $F_{Y,X}$ from $F_X(x) = \int_{y} \frac{\partial^2 F_{Y,X}}{\partial x \partial y}(y,x) dy$. Then,
\begin{align*}
	X_i Y_i &= X_i X_i' \beta + X_i u_i \\
	E[X_i Y_i] &= E[X_i X_i'] \beta + E[X_i u_i] \\
	E[X_i Y_i] &= E[X_i X_i'] \beta &\mbox{(by A1)} \\
	E[X_i X_i']^{-1} E[X_i Y_i] &= \beta &\mbox{(by A2)}
\end{align*}
And so $\beta$ is point identified. Finally, with $\beta$ identified and using $F_{Y,X}$, one can derive the distribution of $F_{u|X}(\cdot)$ by
\begin{align*}
	F_{u|X}(t) &= P(u_i < t | X_i) \\
	&= P(Y_i - X_i'\beta < t | X_i) \\
	&= P(Y_i < t + X_i'\beta | X_i) \\
	&= \int_{y < t + X_i'\beta} dF_{Y|X}(y|x) dy
\end{align*}
where $F_{Y|X}$ is the conditional c.d.f. of $Y$ given $X$ implied by the j.d.f. of $Y$ and $X$.

\subsection*{Identification II (A Structural Labor Supply Model)}

\begin{enumerate}[a)]

	\item Derive the payoffs to working 20 and 40 hours:
	\begin{align*}
		u_{20}(w_i;d_i,a) &= 20 w_i - \frac{d_i}{1+a} 20^{1+a} \\
		u_{40}(w_i;d_i,a) &= 40 w_i - \frac{d_i}{1+a} 40^{1+a}
	\end{align*}

	\item Derive the probability of working 40 hours a week given wages:

	Given wages, the worker will work 40 hours if $u_{40}(w_i;d_i,a) > u_{20}(w_i;d_i,a)$, which holds when
	\begin{align*}
		40 w_i - \frac{d_i}{1+a} 40^{1+a} &> 20 w_i - \frac{d_i}{1+a} 20^{1+a} \\
		2 w_i - \frac{2d_i}{1+a} 40^{a} &> w_i - \frac{d_i}{1+a} 20^{a} \\
		w_i (1+a) &> 2d_i 40^a- d_i 20^{a} \\
		\implies d_i &< \frac{w_i (1+a)}{2 \cdot 40^a - 20^a} \\
		\implies \log d_i &< \log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a)
	\end{align*}
	The probability of which is $\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$, where $\Phi$ is the c.d.f. of the standard normal.

	\item Derive an individual's contribution to the likelihood of the observed data: Given an observation $(h_i, w_i)$ under a given parameter $a$ and parameters on the distribution of $\log d_i$, the likelihood of the observed data is $\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$ if $h_i = 40$ and $1 - \Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$ if $h_i = 20$.

	\item These likelihoods are, naturally, used in the maximum likelihood estimator.

	\item The parameters $(a, \mu, \sigma)$ are set-identified (any two will imply the third).

	\item Derive an expression for the probability of working 20 and 40 hours a week when the choice set for hours is $\{ 10,20,40 \}$: The utility from working 20 and 40 hours is as above, and the utility of working 10 hours is
	\begin{align*}
		u_{10}(w_i;d_i,a) &= 10 w_i - \frac{d_i}{1+a} 10^{1+a}
	\end{align*}
	A worker works 40 hours when $u_{40} > u_{20}$ (as above) and $u_{40} > u_{10}$. The latter happens when
	\begin{align*}
		40 w_i - \frac{d_i}{1+a} 40^{1+a} &> 10 w_i - \frac{d_i}{1+a} 10^{1+a} \\
		4 w_i - \frac{4d_i}{1+a} 40^{a} &> w_i - \frac{d_i}{1+a} 10^{a} \\
		3 w_i (1+a) - 4d_i 40^{a} &> - d_i 10^{a} \\
		\implies d_i &< \frac{3w_i(1+a)}{4 \cdot 40^a - 10^a}
	\end{align*}
	However, this condition is met when $u_{40} > u_{20}$, unless $a < 0$ which is outside of the parameter space. So, the probability that the worker chooses 40 hours the same as in the $\{20,40\}$ choice set.

	Using a similar derivation to the above, the worker chooses 20 hours over 10 hours when $d_i < \frac{w_i (1+a)}{2 \cdot 20^a - 10^a}$. So, the probability that the worker chooses 20 hours is
	\begin{align*}
		\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 20^a - 10^a) - \mu}{\sigma} \bigg)  \\ - \Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)
	\end{align*}
	
	\item With the addition of the option to work 10 hours, the parameters are still set-identified, in the sense that choosing one value would imply the others.

\end{enumerate}

\subsection*{Identification III (Mixture of Normals)}
We can write this problem as 
$$Y_i = D_i X_i + (1-D_i)W_i \text{ with } D_i \times X_i \times W_i \sim Bernoulli(\rho) \times N(0,1) \times N(0,\sigma^2).$$ 

The structure is thus pinned down by the parameters $\mu = (\sigma^2,\rho)$. A structure in this model space that is not identified--either globally or locally--is $\mu' = (1,\rho')$ for some $\rho \in \mathbb{R}$.

By the law of total probability, $$f_{\mu'}(y) = \rho' \phi(y) + (1-\rho')\phi(y) = \phi(y) = \tilde \rho \phi(y) + (1-\tilde \rho)\phi(y) = f_{\tilde \mu}(y)$$ for some $\tilde \rho \in B_{\epsilon}(\rho')$, where $B_{\epsilon}(\rho')$ is an epsilon ball around the true parameter $\rho'$. 

Thus, for any value of $\rho$, if the variance of $W_i$ is equal to $1$, we cannot identify the structure generating the observed data.

\subsection*{Quantile Treatment Effects}

\begin{enumerate}[a)]

	\item Let $Z_i = g(Y_i)$, where $g(\cdot)$ is a monotonically increasing function. Prove that $Q_{\tau}(Z_i) = g(Q_{\tau}(Y_i))$:

	\begin{align*}
		F_Z(g(Q_{\tau}(Y_i))) &= Pr(Z_i < g(Q_{\tau}(Y_i))) \\
		&= Pr(Y_i < Q_{\tau}(Y_i)) &\mbox{(Since $g(\cdot)$ is monotonically increasing)} \\
		&= F_Y(Q_{\tau}(Y_i)) = \tau \\
		\implies g(Q_{\tau}(Y_i)) &= F_Z^{-1}(\tau) = Q_{\tau}(Z_i) &\square
	\end{align*}
	

\end{enumerate}


\subsection*{Iterated Projections}
Prove the law of iterated projections 
$$E^*[Y_i|X_i] = E^*[E^*[Y_i|X_i,Z_i]|X_i]$$

\begin{proof}\mbox{}\\
	Define $W_i' = \begin{bmatrix}X_i' & Z_i'\end{bmatrix}$. Recall that the linear projection of $W_i$ onto $Y_i$ requires that, for $\beta = E[W_i W_i']^{-1}E[W_i Y_i]$, the following must hold:

	$$E[W_i(Y_i - W_i'\beta)] = 0.$$

	This further implies that $$E[X_i\underbrace{(Y_i - W_i'\beta)}_{\equiv u_i}] = 0$$

	Thus,
	\begin{align*}
		E^*[Y_i|X_i] & = X_i'E[X_iX_i']E[X_iY_i] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i(W_i'\beta + u_i)] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i W_i'\beta] + X_i'E[X_iX_i']^{-1}E[X_i u_i] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i E^*[Y_i|X_i,Z_i]] \\
		& = E^*[E^*[Y_i|X_i,Z_i]|X_i]
	\end{align*}
	The third equality follows from the linearity of the expectation operator. The second term in the third equality is equal to zero by the observation made above. The fourth follows from the definition $E^*[Y_i|X_i,Z_i] = W_i\beta$.
	\end{proof}

\subsection*{FWL Theorem}
Suppose the population projection $E^*[Y_i|X_i,Z_i] = X_i'\beta + Z_i'\gamma$. Let $\tilde Z_i = Z_i - E^*[Z_i|X_i]$ and $\tilde Y_i = Y_i - E^*[Y_i|X_i]$:

\begin{enumerate}[label = (\alph*)]
	\item Show that $\gamma = E\left[\tilde Z_i \tilde Z_i'\right]^{-1} E\left[\tilde Z_i \tilde Y_i\right]$
	\begin{proof}\mbox{}\\
		Observe that we can write $Y_i = X_i'\beta + Z_i'\gamma + \epsilon_i$ where $\epsilon_i$ is the conditional expectation error term from the projection of $X_i$ and $Z_i$ onto $Y_i$. Recall that by definition, $E[X_i \epsilon_i] = E[Z_i \epsilon_i] = 0$. Thus, we can write:
		\begin{align*}
			Y_i & = X_i'\beta + Z_i'\gamma + \epsilon_i \\
			X_i Y_i & = X_i X_i'\beta + X_i Z_i'\gamma + X_i \epsilon_i \\
			E[X_i Y_i] & = E[X_i X_i'\beta] + E[X_i Z_i'\gamma] + E[X_i \epsilon_i]\\
			X_i'E[X_iX_i']^{-1}E[X_i Y_i] & = X_i'E[X_iX_i']^{-1}E[X_i X_i']\beta + X_i'E[X_iX_i']^{-1}E[X_i Z_i']\gamma \\
			E^*[Y_i|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma
		\end{align*}
		The third line follows from the first order condition of minimization. The fourth line premultiplies everything by $X_i'E[X_iX_i']^{-1}$. The final line simply replaces the expressions with their definition.

		Now, subtract the final line from the first line:
		\begin{align*}
			\underbrace{Y_i - E^*[Y_i|X_i]}_{\tilde Y_i} & = X_i'\beta - X_i'\beta + \underbrace{Z_i'\gamma - E^*[Z_i|X_i]'\gamma}_{\tilde Z_i'\gamma} + \epsilon_i \\
			\tilde Z_i \tilde Y_i & = \tilde Z_i \tilde Z_i'\gamma + \tilde Z_i\epsilon_i\\
			E[\tilde Z_i \tilde Y_i]& = E[\tilde Z_i \tilde Z_i']\gamma + E[\tilde Z_i\epsilon_i]
		\end{align*}
		Observe that $E[\tilde Z_i\epsilon_i] = 0$ since $\tilde Z_i$ is a linear combination of $Z_i$ and $X_i$, both of which have mean zero correlation with $\epsilon_i$. More precisely, write $E^*[Z_i|X_i] = \Pi X_i$ for some matrix of coefficients of $\Pi$. So,
		$$E[\tilde Z_i \epsilon_i] = E[Z_i \epsilon_i] - E[\Pi X_i\epsilon_i] = 0 - \Pi 0 = 0.$$
		Thus, we have
		$$E[\tilde Z_i \tilde Y_i] = E[\tilde Z_i \tilde Z_i']\gamma,$$
		which after rearranging becomes $\gamma = E[\tilde Z_i \tilde Z_i']^{-1}E[\tilde Z_i \tilde Y_i],$ the desired result.
	\end{proof}
	\item Show that $Y_i - E^*[Y_i|X_i, Z_i] = (Y_i - E^*[Y_i|X_i]) - (Z_i - E^*[Z_i|X_i]'\gamma)$
	\begin{proof}\mbox{}\\
		Observe
		\begin{align*}
			E^*[Y_i|X_i,Z_i] & = X_i'\beta + Z_i'\gamma \\
			E^*[E^*[Y_i|X_i,Z_i]|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma \\
			E^*[Y_i|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma
		\end{align*}
		Now, we have, after solving the above equation for $X_i'\beta$, the desired result:us
		\begin{align*}
		Y_i - X_i'\beta - Z_i'\gamma & = Y_i - E^*[Y_i|X_i] + E^*[Z_i|X_i]'\gamma - Z_i'\gamma \\
		& = (Y_i - E^*[Y_i|X_i]) - (Z_i - E^*[Z_i|X_i])'\gamma
		\end{align*}
	\end{proof}
\end{enumerate}

\subsection*{Weighted Average Derivative Properties}
b. Derive the expression for the weights when $\omega (X_i)$ is normally distributed. \\
\begin{align*}
\omega(X_i) &= ( E[X | X \geq x] - E[X | X < x] ) (P(X \geq x) ( 1 - P(X \geq x)) \\
&=\bigg[ \mu + \sigma \frac{\phi (\frac{x - \mu}{\sigma})}{1 - \Phi( \frac{x - \mu}{\sigma})} - \mu + \sigma \frac{\phi (\frac{x - \mu}{\sigma})}{\Phi( \frac{x - \mu}{\sigma})} \bigg] \bigg[(\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma}) \bigg] \\
&=\sigma \bigg[\frac{ \phi( \frac{x - \mu}{\sigma}) }{\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma})} \bigg] \bigg[\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma})\bigg] \\
&=\sigma \phi( \frac{x - \mu}{\sigma})
\end{align*}
\medskip
Then $\frac{Cov(Y_i, X_i)}{var(X_i)} = E[\mu ' (x)]$


\end{document}