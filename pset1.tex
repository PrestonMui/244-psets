\documentclass[11pt]{article}
\usepackage{minibox}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{ bbold }
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#1 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\subsection*{Identification I (OLS)}

Prove that $\theta \equiv \{ \beta, F_X(\cdot), F_{u|X}(\cdot) \}$ is point identified in OLS:

First, $F_X(\cdot)$ is point identified from the joint distribution of $F_{Y,X}$ from $F_X(x) = \int_{y} \frac{\partial^2 F_{Y,X}}{\partial x \partial y}(y,x) dy$. Then,
\begin{align*}
	X_i Y_i &= X_i X_i' \beta + X_i u_i \\
	E[X_i Y_i] &= E[X_i X_i'] \beta + E[X_i u_i] \\
	E[X_i Y_i] &= E[X_i X_i'] \beta &\mbox{(by A1)} \\
	E[X_i X_i']^{-1} E[X_i Y_i] &= \beta &\mbox{(by A2)}
\end{align*}
And so $\beta$ is point identified. Finally, with $\beta$ identified and using $F_{Y,X}$, one can derive the distribution of $F_{u|X}(\cdot)$ by
\begin{align*}
	F_{u|X}(t) &= P(u_i < t | X_i) \\
	&= P(Y_i - X_i'\beta < t | X_i) \\
	&= P(Y_i < t + X_i'\beta | X_i) \\
	&= \int_{y < t + X_i'\beta} dF_{Y|X}(y|x) dy
\end{align*}
where $F_{Y|X}$ is the conditional c.d.f. of $Y$ given $X$ implied by the j.d.f. of $Y$ and $X$.

\subsection*{Identification II (A Structural Labor Supply Model)}

\begin{enumerate}[a)]

	\item Derive the payoffs to working 20 and 40 hours:
	\begin{align*}
		u_{20}(w_i;d_i,a) &= 20 w_i - \frac{d_i}{1+a} 20^{1+a} \\
		u_{40}(w_i;d_i,a) &= 40 w_i - \frac{d_i}{1+a} 40^{1+a}
	\end{align*}

	\item Derive the probability of working 40 hours a week given wages:

	Given wages, the worker will work 40 hours if $u_{40}(w_i;d_i,a) > u_{20}(w_i;d_i,a)$, which holds when
	\begin{align*}
		40 w_i - \frac{d_i}{1+a} 40^{1+a} &> 20 w_i - \frac{d_i}{1+a} 20^{1+a} \\
		2 w_i - \frac{2d_i}{1+a} 40^{a} &> w_i - \frac{d_i}{1+a} 20^{a} \\
		w_i (1+a) &> 2d_i 40^a- d_i 20^{a} \\
		\implies d_i &< \frac{w_i (1+a)}{2 \cdot 40^a - 20^a} \\
		\implies \log d_i &< \log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a)
	\end{align*}
	The probability of which is $\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$, where $\Phi$ is the c.d.f. of the standard normal.

	\item Derive an individual's contribution to the likelihood of the observed data: Given an observation $(h_i, w_i)$ under a given parameter $a$ and parameters on the distribution of $\log d_i$, the likelihood of the observed data is $\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$ if $h_i = 40$ and $1 - \Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)$ if $h_i = 20$.

	\item These likelihoods are, naturally, used in the maximum likelihood estimator.

	\item The parameters $(a, \mu, \sigma)$ are set-identified (any two will imply the third).

	\item Derive an expression for the probability of working 20 and 40 hours a week when the choice set for hours is $\{ 10,20,40 \}$: The utility from working 20 and 40 hours is as above, and the utility of working 10 hours is
	\begin{align*}
		u_{10}(w_i;d_i,a) &= 10 w_i - \frac{d_i}{1+a} 10^{1+a}
	\end{align*}
	A worker works 40 hours when $u_{40} > u_{20}$ (as above) and $u_{40} > u_{10}$. The latter happens when
	\begin{align*}
		40 w_i - \frac{d_i}{1+a} 40^{1+a} &> 10 w_i - \frac{d_i}{1+a} 10^{1+a} \\
		4 w_i - \frac{4d_i}{1+a} 40^{a} &> w_i - \frac{d_i}{1+a} 10^{a} \\
		3 w_i (1+a) - 4d_i 40^{a} &> - d_i 10^{a} \\
		\implies d_i &< \frac{3w_i(1+a)}{4 \cdot 40^a - 10^a}
	\end{align*}
	However, this condition is met when $u_{40} > u_{20}$, unless $a < 0$ which is outside of the parameter space. So, the probability that the worker chooses 40 hours the same as in the $\{20,40\}$ choice set.

	Using a similar derivation to the above, the worker chooses 20 hours over 10 hours when $d_i < \frac{w_i (1+a)}{2 \cdot 20^a - 10^a}$. So, the probability that the worker chooses 20 hours is
	\begin{align*}
		\Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 20^a - 10^a) - \mu}{\sigma} \bigg)  \\ - \Phi \bigg( \frac{\log w_i + \log (1+a) - \log(2 \cdot 40^a - 20^a) - \mu}{\sigma} \bigg)
	\end{align*}
	
	\item With the addition of the option to work 10 hours, the parameters are still set-identified, in the sense that choosing one value would imply the others.

\end{enumerate}

\subsection*{Identification III (Mixture of Normals)}
We can write this problem as 
$$Y_i = D_i X_i + (1-D_i)W_i \text{ with } D_i \times X_i \times W_i \sim Bernoulli(\rho) \times N(0,1) \times N(0,\sigma^2).$$ 

The structure is thus pinned down by the parameters $\mu = (\sigma^2,\rho)$. A structure in this model space that is not identified--either globally or locally--is $\mu' = (1,\rho')$ for some $\rho \in \mathbb{R}$.

By the law of total probability, $$f_{\mu'}(y) = \rho' \phi(y) + (1-\rho')\phi(y) = \phi(y) = \tilde \rho \phi(y) + (1-\tilde \rho)\phi(y) = f_{\tilde \mu}(y)$$ for some $\tilde \rho \in B_{\epsilon}(\rho')$, where $B_{\epsilon}(\rho')$ is an epsilon ball around the true parameter $\rho'$. 

Thus, for any value of $\rho$, if the variance of $W_i$ is equal to $1$, we cannot identify the structure generating the observed data.

\subsection*{Quantile Treatment Effects}

\begin{enumerate}[a)]

	\item Let $Z_i = g(Y_i)$, where $g(\cdot)$ is a monotonically increasing function. Prove that $Q_{\tau}(Z_i) = g(Q_{\tau}(Y_i))$:

	\begin{align*}
		F_Z(g(Q_{\tau}(Y_i))) &= Pr(Z_i < g(Q_{\tau}(Y_i))) \\
		&= Pr(Y_i < Q_{\tau}(Y_i)) &\mbox{(Since $g(\cdot)$ is monotonically increasing)} \\
		&= F_Y(Q_{\tau}(Y_i)) = \tau \\
		\implies g(Q_{\tau}(Y_i)) &= F_Z^{-1}(\tau) = Q_{\tau}(Z_i) &\square
	\end{align*}

	\item Consider an experiment with a treatment variable $T_i \in \{0,1\}$ and an observed outcome $Y_i$. Let $Y_i(1)$ and $Y_i(0)$ denote potential values of $Y_i$ with and without treatment. Define the $\tau-th$ quantile treatment effect
	$$QTE(\tau) = Q_\tau(Y_i(1)) - Q_\tau(Y_i(0))$$

	Assume that potential outcomes are independent of $T_i$. Show that $QTE(\tau)$ is identified for every $\tau$.

	\begin{proof}\mbox{}\\
		Fix $\tau$. Consider $Q_\tau(Y_i(1)|T_i = 1)$, the quantile of potential outcomes for the treatment, conditional on treatment. By definition (with $t$ as the indicator variable for integration), we have
		\begin{align*}
			\tau & = \int_{-\infty}^{Q_\tau(Y_i(1)|T_i = 1)}t dF_{Y_i(1)|T_i = 1}(t|T_i = 1) \\
			\tau & = \int_{-\infty}^{Q_\tau(Y_i(1)|T_i = 1)}t dF_{Y_i(1)}(t) &\mbox{By independence of the treatment}\\
			\Longrightarrow & \\
			& Q_\tau(Y_i(1)|T_i = 1) = Q_\tau(Y_i(1))
		\end{align*} 
		A similar argument holds for $Q_\tau(Y_i(0))$. Thus, we have
		$$ Q_\tau(Y_i(1)|T_i = 1) - Q_\tau(Y_i(0)|T_i = 0) = Q_\tau(Y_i(1)) - Q_\tau(Y_i(0)) = QTE(\tau)$$
	\end{proof}

	\item Suppose the following condition holds with probability one:
	$$ F_1(Y_i(1)) = F_0(Y_i(0)),$$
	where $F_t(y)$ is the CDF of $Y_i(t)$. Let $\delta_i = Y_i(1) - Y_i(0)$. Show that $Q_\tau(\delta_i)$ is identified for every $\tau$. Comment on the interpretation of $Q_\tau(\delta_i)$.

	\begin{proof}\mbox{}\\
		We can write the condition provided in this problem as 
		$$Pr(F_1(Y_i(1)) - F_0(Y_i(0)) < \epsilon) = 1 \mbox{ } \forall \epsilon > 0$$

		Let $Y_i^*(1)$ be an element in the support of $Y_i(1)$. Set $\tau = F_1(Y_i^*(1))$, so that $Q_\tau(Y_i(1)) = Y_i^*(1)$. Then, define $Y_i^*(0) \equiv Q_\tau(Y_i(0))$. Then, the condition above is equivalent to $$\delta_i^* = Y_i^*(1) - Y_i^*(0)$$ holding with probability one. 

		Since $Y_i^*(0)$ is implicitly defined by our choice of $Y_i^*(1)$, we have that $\delta_i^* = h(Y_i^*(1))$ with probability one for some function $h:\mathbb{S}(Y_i(1)) \to \mathbb{R}$, where $\mathbb{S}(Y_i(1))$ is the support of the random variable $Y_i(1)$.

		Denote $\tilde \tau(x) = F_{Y_i(1)}(x)$ as the the cdf of $Y_i(1)$ evaluated at $x$. 

		The cumulative density function of $\delta_i$ can thus be defined as
		$$F_{\delta_i}(X) = \int_{-\infty}^{\infty}t \cdot \mathbb{1}(\underbrace{Q_{\tilde \tau(y_1)}(Y_i(1))}_{= y_1^*} - \underbrace{Q_{\tilde \tau(y_1)}(Y_i(0))}_{= y_0^*} < X)dF_{Y_i(1)}(t)$$

		Then, define $Q_\tau(\delta_i) \equiv F_{\delta_i}^{-1}(\tau)$. The idea is that the density of observing $\delta_i^*$ equal to the density associated with $Y_i^*(1)$.
	\end{proof}
	
	\item You have an $iid$ sample $\{Y_i,T_i\}_{i=1}^{N}$ that satisfies the assumptions in parts $b)$ and $c)$. Propose an estimator of $Q_\tau(\delta_i)$.

	In the previous problem, observe that the term within the $\mathbb{1}(\cdot)$ expression corresponds to the QTE, evaluated at $\tau(y_1)$, which we can estimate by comparing $Q_t(Y_i(1)|T_i = 1)$ with $Q_t(Y_i(0)|T_i = 0)$. 

	Let $J$ be the number of observations such that $T_i = 1$, so that the number of untreated observations is $N - J$. Without loss of generality, order $Y_i(1)$ such that $i = 1,2,...,J$ correspond to the treated group and $Y_{j-1} < Y_{j}$. Similarly order the untreated groups for $i > J$.\footnote{Here we are using $Y_i(1)$ as shorthand for $\{ Y_i,T_i = 1\}$}

	Define the empirical estimator of the quantile function as
	$$\hat Q_{\tau}(Y_i(1)) = \text{inf}\left\{ Y_i(1) | \tau \leq \frac{i}{J} \right\}$$Similarly, define
	$$\hat Q_{\tau}(Y_i(0)) = \text{inf}\left\{ Y_i(0) | \tau \leq \frac{i-J}{N-J} \right\}$$

	One possible estimator of $F_\delta$ is as follows:
	$$\hat F_\delta (X) = \sum_{i=1}^J \mathbb{1}\big(Y_i -  \hat Q_{\frac{i}{J}}(Y_i(0)) < X\big)\frac{i}{J}$$

	Then, we define the empricial estimator $\hat Q_{\delta}(\tau) = \text{inf}\left\{ X |\tau \leq \hat F_\delta (X)\right\}$
\end{enumerate}


\subsection*{Iterated Projections}
Prove the law of iterated projections 
$$E^*[Y_i|X_i] = E^*[E^*[Y_i|X_i,Z_i]|X_i]$$

\begin{proof}\mbox{}\\
	Define $W_i' = \begin{bmatrix}X_i' & Z_i'\end{bmatrix}$. Recall that the linear projection of $W_i$ onto $Y_i$ requires that, for $\beta = E[W_i W_i']^{-1}E[W_i Y_i]$, the following must hold:

	$$E[W_i(Y_i - W_i'\beta)] = 0.$$

	This further implies that $$E[X_i\underbrace{(Y_i - W_i'\beta)}_{\equiv u_i}] = 0$$

	Thus,
	\begin{align*}
		E^*[Y_i|X_i] & = X_i'E[X_iX_i']E[X_iY_i] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i(W_i'\beta + u_i)] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i W_i'\beta] + X_i'E[X_iX_i']^{-1}E[X_i u_i] \\
		& = X_i'E[X_iX_i']^{-1}E[X_i E^*[Y_i|X_i,Z_i]] \\
		& = E^*[E^*[Y_i|X_i,Z_i]|X_i]
	\end{align*}
	The third equality follows from the linearity of the expectation operator. The second term in the third equality is equal to zero by the observation made above. The fourth follows from the definition $E^*[Y_i|X_i,Z_i] = W_i\beta$.
	\end{proof}

\subsection*{FWL Theorem}
Suppose the population projection $E^*[Y_i|X_i,Z_i] = X_i'\beta + Z_i'\gamma$. Let $\tilde Z_i = Z_i - E^*[Z_i|X_i]$ and $\tilde Y_i = Y_i - E^*[Y_i|X_i]$:

\begin{enumerate}[label = (\alph*)]
	\item Show that $\gamma = E\left[\tilde Z_i \tilde Z_i'\right]^{-1} E\left[\tilde Z_i \tilde Y_i\right]$
	\begin{proof}\mbox{}\\
		Observe that we can write $Y_i = X_i'\beta + Z_i'\gamma + \epsilon_i$ where $\epsilon_i$ is the conditional expectation error term from the projection of $X_i$ and $Z_i$ onto $Y_i$. Recall that by definition, $E[X_i \epsilon_i] = E[Z_i \epsilon_i] = 0$. Thus, we can write:
		\begin{align*}
			Y_i & = X_i'\beta + Z_i'\gamma + \epsilon_i \\
			X_i Y_i & = X_i X_i'\beta + X_i Z_i'\gamma + X_i \epsilon_i \\
			E[X_i Y_i] & = E[X_i X_i'\beta] + E[X_i Z_i'\gamma] + E[X_i \epsilon_i]\\
			X_i'E[X_iX_i']^{-1}E[X_i Y_i] & = X_i'E[X_iX_i']^{-1}E[X_i X_i']\beta + X_i'E[X_iX_i']^{-1}E[X_i Z_i']\gamma \\
			E^*[Y_i|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma
		\end{align*}
		The third line follows from the first order condition of minimization. The fourth line premultiplies everything by $X_i'E[X_iX_i']^{-1}$. The final line simply replaces the expressions with their definition.

		Now, subtract the final line from the first line:
		\begin{align*}
			\underbrace{Y_i - E^*[Y_i|X_i]}_{\tilde Y_i} & = X_i'\beta - X_i'\beta + \underbrace{Z_i'\gamma - E^*[Z_i|X_i]'\gamma}_{\tilde Z_i'\gamma} + \epsilon_i \\
			\tilde Z_i \tilde Y_i & = \tilde Z_i \tilde Z_i'\gamma + \tilde Z_i\epsilon_i\\
			E[\tilde Z_i \tilde Y_i]& = E[\tilde Z_i \tilde Z_i']\gamma + E[\tilde Z_i\epsilon_i]
		\end{align*}
		Observe that $E[\tilde Z_i\epsilon_i] = 0$ since $\tilde Z_i$ is a linear combination of $Z_i$ and $X_i$, both of which have mean zero correlation with $\epsilon_i$. More precisely, write $E^*[Z_i|X_i] = \Pi X_i$ for some matrix of coefficients of $\Pi$. So,
		$$E[\tilde Z_i \epsilon_i] = E[Z_i \epsilon_i] - E[\Pi X_i\epsilon_i] = 0 - \Pi 0 = 0.$$
		Thus, we have
		$$E[\tilde Z_i \tilde Y_i] = E[\tilde Z_i \tilde Z_i']\gamma,$$
		which after rearranging becomes $\gamma = E[\tilde Z_i \tilde Z_i']^{-1}E[\tilde Z_i \tilde Y_i],$ the desired result.
	\end{proof}
	\item Show that $Y_i - E^*[Y_i|X_i, Z_i] = (Y_i - E^*[Y_i|X_i]) - (Z_i - E^*[Z_i|X_i]'\gamma)$
	\begin{proof}\mbox{}\\
		Observe
		\begin{align*}
			E^*[Y_i|X_i,Z_i] & = X_i'\beta + Z_i'\gamma \\
			E^*[E^*[Y_i|X_i,Z_i]|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma \\
			E^*[Y_i|X_i] & = X_i'\beta + E^*[Z_i|X_i]'\gamma
		\end{align*}
		Now, we have, after solving the above equation for $X_i'\beta$, the desired result:us
		\begin{align*}
		Y_i - X_i'\beta - Z_i'\gamma & = Y_i - E^*[Y_i|X_i] + E^*[Z_i|X_i]'\gamma - Z_i'\gamma \\
		& = (Y_i - E^*[Y_i|X_i]) - (Z_i - E^*[Z_i|X_i])'\gamma
		\end{align*}
	\end{proof}
\end{enumerate}

\subsection*{Weighted Average Derivative Properties}

\begin{enumerate}[a)]

	\item The covariance of $X_i$ and $Y_i$ is given by

	\begin{align*}
		E[(Y_i - \bar{Y})(X_i - \bar{X})] &=  E[Y_i(X_i - \bar{X}) - \bar{Y}(X_i - \bar{X})] \\
		&= E[Y_i(X_i - \bar{X})]- E[\bar{Y}(X_i - \bar{X})] \\
		&= E[Y_i(X_i - \bar{X})] \\
		&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} Y_i (X_i - \bar{X}) f_{y|x}(Y_i|X_i)dy f(x)dx \\
		&= \int_{-\infty}^{\infty} \mu(X_i) (X_i - \bar{X}) f(x)dx \\
	\end{align*}
	Where $f(x)$ is the p.d.f. of $X_i$. Since $\mu$ is continuous and differentiable, the fundamental theorem of calculus implies
	\begin{align*}
		\mu(X_i) &= \kappa + \int_{-\infty}^{X_i}\mu'(t) dt
	\end{align*}
	where $\kappa = \lim_{X_i \xrightarrow -\infty} \mu(X_i)$. Substituting this in for $\mu(X_i)$,
	\begin{align*}
		&= \int_{-\infty}^{\infty} \Big( \kappa + \int_{-\infty}^{X_i}\mu'(t) dt \Big) (X_i - \bar{X}) f(x) dx \\
		&= \int_{-\infty}^{\infty} \int_{-\infty}^{X_i}\mu'(t) (X_i - \bar{X}) dt f(x)dx \\
		&= \int_{-\infty}^{\infty} \mu'(t) \int_{t}^{\infty } (X_i - \bar{X}) f(x)dxdt
	\end{align*}
	The inner integral is
	\begin{align*}
		\int_{t}^{\infty} (X_i - \bar{X}) dx &= \int_{t}^{\infty} \bigg( X_i - \int_{-\infty}^{\infty} u f(u)du \bigg) f(x)dx \\
		&= \int_{t}^{\infty} X_i f(x)dx - \int_{t}^{\infty} \int_{-\infty}^{\infty} u f(u)du f(x)dx \\
		&= \int_{t}^{\infty} X_i f(x)dx - \int_{t}^{\infty} \bigg( \int_{-\infty}^{t} u f(u)du + \int_{t}^{\infty} u f(u)du \bigg) f(x)dx \\
		&= E[X_i | X_i > t] \rho_t - \rho_t \bigg( E[X_i | X_i < t] (1 - \rho_t) + E[X_i | X_i > t] \rho_t \bigg) &\mbox{$\rho_t = P(X_i > t)$} \\
		&= \rho_t(1-\rho_t) (E[X_i | X_i > t] - E[X_i | X_i < t]) \equiv \omega(t)
	\end{align*}
	And so the covariance is
	\begin{align*}
		Cov(X_i,Y_i) &= \int_{-\infty}^{\infty} \mu'(t) \omega(t) dt
	\end{align*}
	and
	\begin{align*}
		Var(X_i) = Cov(X_i,X_i) &= \int_{-\infty}^{\infty} 1 \cdot \omega(t) dt = \int_{-\infty}^{\infty} \omega(t) dt &\square
	\end{align*}	

	\item Derive the expression for the weights when $\omega (X_i)$ is normally distributed.

	\begin{align*}
	\omega(X_i) &= ( E[X | X \geq x] - E[X | X < x] ) (P(X \geq x) ( 1 - P(X \geq x)) \\
	&=\bigg[ \mu + \sigma \frac{\phi (\frac{x - \mu}{\sigma})}{1 - \Phi( \frac{x - \mu}{\sigma})} - \mu + \sigma \frac{\phi (\frac{x - \mu}{\sigma})}{\Phi( \frac{x - \mu}{\sigma})} \bigg] \bigg[\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma})) \bigg] \\
	&=\sigma \bigg[\frac{ \phi( \frac{x - \mu}{\sigma}) }{\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma}))} \bigg] \bigg[\Phi( \frac{x - \mu}{\sigma})(1- \Phi( \frac{x - \mu}{\sigma}))\bigg] \\
	&=\sigma \phi( \frac{x - \mu}{\sigma})
	\end{align*}
	\medskip
	Then $\frac{Cov(Y_i, X_i)}{var(X_i)} = E[\mu ' (x)]$

\end{enumerate}

7c. Prove that for the scalar random variables Y and S and a vector $X_i$ \\
	\begin{align*}
		\frac{E[Y_i(S_i - E[S_i|X_i])]}{E[S_i(S_i - E[S_i|X_i])]} = \frac{E [\int \mu'_X(t) 		\omega_X(t) dt]}{E[\int \omega_X(t) dt]}
		\end{align*}
where $\mu_X(t) = E[Y_i|X_i, S_i=t]$. What is $\omega_X(t)$? \\
\medskip

\textit{Proof} \\
Focusing on the numerator:
	\begin{align*}
		E[Y_i(S_i - E[S_i|X_i])] &= E[E[Y_i|S_i, X_i] (S_i - E[S_i|X_i])]
	\end{align*}
And $E[Y_i|X_i, S_i=t]=\mu_{X} = c + \int^{X_i}_{-\infty} \mu'_X(t) dt$  by the Fundamental Theorem of Calculus, so the expression becomes: 
	\begin{align*}
		&= E[c+ (\int^{S_i}_{-\infty} \mu'_X(t) dt)  (S_i - E[S_i|X_i])] \\
		&= E[ E[(\int^{S_i}_{-\infty} \mu'_X(t) dt (S_i - E[S_i|X_i]) | X_i] ]\\
		&=E[ \int^{+\infty}_{-\infty}\int^{S_i}_{-\infty} \mu'_X(t) dt)  (S_i - E[S_i|X_i])]f_{S|X}ds ]
	\end{align*}
Reversing the order of integration: 
	\begin{align}
		&=E[ \int^{+\infty}_{-\infty} \mu'_X(t) \int^{+ \infty}_{t} ( S_i - E[S_i|X_i]) f_{S|X} ds dt ]
	\end{align}
Focusing on the inner integral:
	\begin{align}
		\int^{+ \infty}_{t} ( S_i - E[S_i|X_i]) f_{S|X} ds &= \int^{+ \infty}_{t} S_i f_{S|X} ds - \int^{+ \infty}_{t} E[S_i|X_i] f_{S|X} ds \\
		&=(E[S_i |  X_i, S_i \geq t] - E[S_i | X_i] ) P(S_i \geq t | X_i) 
	\end{align}		
We can rewrite $E[S_i|X_i]$ as $E[S_i|X_i, S_i \geq t]P[S_i\geq t|X_i] + E[S_i|X_i, S_i < t](1 - P[S_i\geq t|X_i])$. Plugging this in for $E[S_i | X_i]$ then (3) becomes:
	\begin{align}
		&=(E[S_i | S_i \geq t, X_i ] - E[S_i | S_i < t, X_i ] )(P(S \geq t | X_i)( 1 - P(S \geq t | X_i)) = \omega_X(t)
	\end{align}
Going back to the full expression in (1), plugging in (4):
	\begin{align*}
		 &=E [\int \mu'_X(t) \omega_X(t) dt]
	\end{align*}
\\
For the denominator, we have a parallel set up except $E[S_i| X_i, S_i=t]$, the derivative of which is 1, so: 
	\begin{align*}
		\frac{E[Y_i(S_i - E[S_i|X_i])]}{E[S_i(S_i - E[S_i|X_i])]} = \frac{E [\int \mu'_X(t) 		\omega_X(t) dt]}{E[\int \omega_X(t) dt]}
	\end{align*}
	where $\omega_X(t) = ( E[S_i | X_i, S_i \geq t ] -E[S_i | X_i, S_i < t])(P[S_i\geq t| X_i])(1 -  P[S_i\geq t| X_i])$ 


\end{document}