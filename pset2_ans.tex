\documentclass[11pt]{article}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate,amsmath,amsthm,amssymb,bbold}
\usepackage{minibox,graphicx,caption,booktabs,pdflscape,multirow,verbatim,subcaption,pdfpages,longtable}
\usepackage[shortlabels]{enumitem}
\usepackage[T1]{fontenc}
\usepackage[section]{placeins} %to keep graphs and tables in the very section to which they belong 
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#2 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\subsection*{1. Problem 1}

\bigskip \textit{Subproblem A}

\bigskip The Delta Method (or the So Called ``Delta Method," so called by Dr. James Powell) says that, if we have a random vector $\theta$ that is asymptotically normal with

\[\sqrt{N}(\hat{\theta}-\theta_0) \overset{d}{\to} N(0,\Sigma)\]

then for some $g(\theta)$ that is continuously differentiable at $\theta=\theta_0$ with Jacobian matrix

\[G_0=\frac{\partial g(\theta_0)}{\partial \theta'}\]

we have that $g(\theta)$ is also asymptotically normal with

\[\sqrt{N}(g(\hat{\theta})-g(\theta_0) \overset{d}{\to} N(0,G_0' \Sigma G_0)\]

In the context of Fehr and Goette (2007), observe that

\[\theta =
\left(
\begin{array}{c}
	\bar{Y_A}\\
	\bar{Y_B}\\
\end{array}
\right)\]

that

\[\Sigma=
\begin{bmatrix}
\sigma_{\bar{Y_A}}^2 & \sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}} & \\
\sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}} & \sigma_{\bar{Y_B}}^2 &
\end{bmatrix}\]

and that

\[g(\theta)=\frac{\bar{Y_A}-\bar{Y_B}}{\bar{Y_B}}\]

We can find these values in the reprinted table. $\bar{Y_A}=4131.33$, $\bar{Y_B}=3005.75$, $\sigma_{\bar{Y_A}}^2=(2669.21)^2$, and $\sigma_{\bar{Y_B}}^2=(2054.20)^2$. But we still have to do a little bit of work.

\bigskip Differentiating $g$ with respect to each of its arguments and evaluating them at $\theta_0$ yields $G$.

\[G =
\left(
\begin{array}{c}
\frac{\partial g}{\partial \bar{Y_A}} = \frac{1}{\bar{Y_B}}\\
\frac{\partial g}{\partial \bar{Y_B}} = \frac{-\bar{Y_A}}{Y_B}\\
\end{array}
\right)
\Rightarrow 
G_0 =
\left(
\begin{array}{c}
\frac{\partial g(\theta_0)}{\partial \bar{Y_A}}=\frac{1}{4131.33}\\
\frac{\partial g(\theta_0)}{\partial \bar{Y_B}}=\frac{-3005.75}{4131.33}\\
\end{array}
\right)
\]

The table also gives us $\sigma_{\bar{Y_A}-\bar{Y_B}}$, which combined with a simple variance identity, will gives us $\sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}}$.

\[VAR(\bar{Y_A}-\bar{Y_B})=VAR(\bar{Y_A})-VAR(\bar{Y_B})-2COV(\bar{Y_A},\bar{Y_B})\]

\[\Rightarrow COV(\bar{Y_A},\bar{Y_B})=\frac{VAR(\bar{Y_A})-VAR(\bar{Y_B})-VAR(\bar{Y_A}-\bar{Y_B})}{2}\]

\[\Rightarrow \sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}}=\frac{(2669.21)^2-(2054.20)^2-(519.72)^2}{2}\]

\[\Rightarrow \sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}}=1317417.75\]

Now we can set up the matrix algebra to use the Delta Method to compute an \textit{estimate} of the standard error, S.

\[S^2 = G_0' \Sigma G_0 = \begin{matrix}\begin{pmatrix}\frac{\partial g}{\partial \bar{Y_A}} & \frac{\partial g}{\partial \bar{Y_B}}\end{pmatrix}\\\mbox{}\end{matrix}
\begin{bmatrix}
\sigma_{\bar{Y_A}}^2 & \sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}} & \\
\sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}} & \sigma_{\bar{Y_B}}^2 &
\end{bmatrix}
\left(\begin{array}{c}
\frac{\partial g}{\partial \bar{Y_A}}\\
\frac{\partial g}{\partial \bar{Y_B}}\\
\end{array}
\right)
\]

\[\Rightarrow S^2 = (\frac{\partial g}{\partial \bar{Y_A}})[(\frac{\partial g}{\partial \bar{Y_A}})(\bar{Y_A}^2)+(\frac{\partial g}{\partial \bar{Y_B}})(\bar{Y_B}^2)] + (\frac{\partial g}{\partial \bar{Y_B}})[(\frac{\partial g}{\partial \bar{Y_A}})(\sigma_{\bar{Y_A}}\sigma_{\bar{Y_B}})+(\frac{\partial g}{\partial \bar{Y_B}})(\sigma_{\bar{Y_B}}^2)] \]

\[\Rightarrow S^2 = \frac{1}{4131.33}[(\frac{1}{4131.33})((2669.21)^2)+(\frac{-3005.75}{4131.33})(1317417.75)]\] \[+\frac{-3005.75}{4131.33}[(\frac{1}{4131.33})(1317417.75)+(\frac{-3005.75}{4131.33})((2054.20)^2)] \]

\[\Rightarrow S = 1494.38\]

\textit{Subproblem B}

\bigskip Let's rename $g$ to be $\eta$. Now we know that

\[\hat{\eta} \sim N(\eta,S) \]

So by the rules of variance,

\[\frac{\hat{\eta}}{.25} \sim N(\frac{\eta}{.25},\frac{S}{(.25)^2}) \]

\[\Rightarrow \frac{\hat{\eta}}{.25} \sim N(\frac{\eta}{.25},\frac{1494.38}{(.25)^2})\]

\[\Rightarrow \frac{\hat{\eta}}{.25} \sim N(\frac{\eta}{.25},23910.08)\]

The experiment's realization of $\hat{\eta}$ is

\[\hat{\eta}=\frac{\bar{Y_A}-\bar{Y_B}}{\bar{Y_B}}=\frac{4131.33-3005.75}{3005.75}=.37\]

Hence producing a 95\% confidence interval for $\frac{\hat{\eta}}{.25}$ is straightforward.

\[\frac{\eta}{.25} \in \left[\frac{.37}{.25}\pm1.96(23910.08)\right] \]

\[\Rightarrow \frac{\eta}{.25} \in \left[-46862.28,46865.24\right]\]

This is a huge confidence interval!

\bigskip This paper published well (in the \textit{AER}) because it was the first to use an RCT to produce clean estimates for a number of labor supply elasticities. But we think the point of this problem is to show that, perhaps because of the small sample size, the appropriate standard error for the revenue/wage elasticity should leave you with little confidence in that particular result.

\subsection*{2. Logit MLE}

\begin{enumerate}[a)]

	\item Derive the score of the logit-likelihood:
	First, noting that
	\begin{align*}
		\frac{\partial \Lambda(X_i'\beta)}{\partial \beta} &= \frac{\exp(X_i' \beta)X_i'(1 + X_i'\beta) - \exp(X_i'\beta) \exp(X_i'\beta) X_i'}{(1+\exp(X_i'\beta))^2} = \frac{\exp(X_i'\beta)X_i'}{(1 + \exp(X_i'\beta))^2}
	\end{align*}
	The score of the logit log-likelihood is therefore
	\begin{align*}
		s(\beta) &= \sum_i \frac{Y_i \exp(X_i'\beta)X_i' }{\Lambda(X_i'\beta)(1 + \exp(X_i'\beta))^2} -
			\frac{(1 - Y_i)\exp(X_i'\beta)X_i'}{(1 - \Lambda(X_i'\beta))(1 + \exp(X_i'\beta))^2} \\
		&= \sum_i \Big( Y_i - (1 - Y_i)\exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)}\\
		&= \sum_i \Big( Y_i(1 + \exp(X_i'\beta)) - \exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)} \\
		&= \sum_i \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} \Big) X_i'
	\end{align*}
	
	\item The moment condition identifying $\beta_{ML}$ is
	\begin{align*}
		E \bigg[ \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)}\Big) X_i' \bigg] &= 0 \\
		E \bigg[ E[Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} | X_i] X_i' \bigg] &= 0
	\end{align*}
	
	\item The moment conditions identifying $\beta_{NLLS}$ are
	\begin{align*}
		0 &= E \bigg[ -2 (Y_i - \Lambda(X_i'\beta)) \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
		&= E \bigg[ E[(Y - \Lambda(X_i'\beta)) | X_i] \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
	\end{align*}

	\item Under which conditions does $\beta_{NLLS}$ coincide with $\beta_{ML}$? The moment conditions will coincide when $E[Y_i - \Lambda(X_i'\beta) | X_i'] = 0$ for all $X_i$; that is, they coincide when the logit model is correctly specified.

\end{enumerate}


\subsection*{3. Matlab Probit DGP}

\begin{enumerate}[a)]

	\item (Matlab program attached)

	\item The ML point estimates of $\hat{\beta}^{con} = (\hat{\alpha},\hat{b})$ are $(-0.0175,0.9159)$. The standard errors are $(0.3030,0.3607)$, respectively.

	\item Score test: Following Wooldridge (2010), page 570, the $LM$ statistic is the ESS from the following regression
	\begin{align*}
		\frac{\hat{u}_i}{\sqrt{\hat{G}_i (1 - \hat{G}_i)}} &= \alpha \frac{\hat{g}_i}{\sqrt{\hat{G}_i(1 - \hat{G}_i)}} x_i + \gamma  \frac{\hat{g}_i}{\sqrt{\hat{G}_i(1 - \hat{G}_i)}} z_i
	\end{align*}
	where $x_i$ is the regressor matrix in the unconstrained regression (constant and $X$) and $z_i$ is the vector of $X_i^2$. The ESS from this regression was $0.2963$, which has a p-value of $0.58621$. So, one does not reject the null that $b_2 = 0$.

	\item The unrestricted model yields point estimates of $\hat{\beta}^{unc} = (\hat{\alpha},\hat{b},\hat{b_2}) = (-0.0435, 0.9175, 0.0428)$. The Wald test will test the null $g(\beta) = 0$ where
	\begin{align*}
		g(\beta) &\equiv (0, 0, 1) \cdot \beta = b_2 \\
		G(\beta) &\equiv \frac{\partial g(\beta)}{\partial \beta} = (0, 0, 1)
	\end{align*}
	and the Wald statistic is given by
	\begin{align*}
		N \cdot \hat{b_2} \cdot \bigg( G \cdot \frac{1}{\sqrt{N}}H^{-1} \cdot G' \bigg)^{-1} \cdot \hat{b_2}
	\end{align*}
	where $H$ is the average Hessian at the ML estimate. Because we know that the model is correctly specified, I use the inverse Hessian instead of the sandwich estimator. The Wald evaluates to 7.4592, which has a p-value of $0.0063$ under the $\chi^2$ distribution with d.f. 1, so one rejects the null that $b_2 = 0$. This is starkly different from the score test result, which did not reject the null. This makes sense, as the Wald tends to reject more than the LM test. If one bumps the number of observations up (say, to 5000), both tests reject the null.

\end{enumerate}

\subsection*{4. Clustered DGP in Stata}

\begin{enumerate}[a)]

	\item Regressing $Y_{ic}$ on $D_{ic}$, we get the results in table 1.
		\input{ps2_4a_regs.tex} 

	The basic SE are 0.03237. The standard errors are slightly larger (0.03244) when we correct for heteroskedasticity in column 2 and significantly larger (0.09508) when we account for the intracluster correlation of observations from the same cluster.

	\item Changing $\sigma^2_{\eta}$ to 0. Regressing $Y_{ic}$ on $D_{ic}$, we get the results in table :
		\input{ps2_4b_regs.tex}

	The standard errors are fairly similar for columns 1 and 2 to what we had in A. They are 0.02732 for the regular SE, 0.027731 for robust SE.  The clustered SE (0.02420) are similar to the magnitude of the non-clustered version.

	\item  Comment on your differences in the answers to a) and b). \\
The SE are fairly similar in columns 1 and 2 for parts a) and b) though they are slightly smaller in part b as a result of the lower variance of $y_{ic}$ in part b. In part b since there is no intra-cluster correlation in the coefficient on $d_{ic}$ the clustered standard errors are of a similar magnitude to the non-clustered version (0.02420). 

	\item In table 3, we see that the simulation rejects the null that the coefficient on $d_{ic}$=1 57\% of the time when we don't cluster our standard errors and about 5\% (which we would expect) when we cluster. So the non-clustered version is rejecting too often and the clustered version is rejecting at about the rate of p which we set.
		\input{ps2_4d_regs.tex}

	\item Here we have that the regression with non-clustered standard errors rejects the test that the coefficient equals the average of the cluster level $\eta$'s 3\% of the time and the clustered standard errors are never rejecting, so in this case clustering is causing us not reject often enough. \\
		\input{ps2_4e_regs.tex}

	\item TBD
\end{enumerate}


\newpage
\subsection*{5. CPS and WLS}

\begin{enumerate}[a)]

	\item See results in column 1 of table 5. 
		\input{ps2_5a.tex}

	\item Collapsed results used in analysis below.

	\item See column 2 of table 5. The coefficients are identical (as we would expect) but the standard errors are much smaller when we weight by bin size. 

	\item If wages are $iid$ within a cell, then the variance of each cell is $\sigma^2 / N_c$ where $\sigma^2$ is the population variance and $N_c$ is the number of observations in the cell.
 
	\item Formula above is used to calculate the variance of the cell means.

	\item See results in column 3 of table 5. 

	\item With an RSS of 0.133 and 38 degrees of freedom we are not able to reject this model at the 5\% level. In other words we cannot reject our model explains the data up to a sampling error. 

	\item See results in table 7. 
		\input{ps2_5h.tex}
	\item With an RSS of 0.009 and 15 degrees of freedom we cannot reject this goodness of fit of this model at the 5\% level either. 
	\item See fig. \ref{ps2_5j}. The model does a very nice job of fitting the data. The place it has a bit of trouble fitting is for older more educated individuals. The model overestimates the wages of 65+ males with some college and underestimates the wages for males with a BA or more. 

		\begin{figure}
			\begin{center}
				\caption{Predicted and actual log wages by subgroup} 	\label{ps2_5j}
				\includegraphics[scale=2]{ps2_5j.pdf}
			\end{center}
		\end{figure}

	\item TBD
\end{enumerate}

\subsection*{6. Random Coefficient Binary Choice Model:}
\begin{enumerate}[a)]
	\item As was derived in the lecture notes, the simulated log likelihood can be derived as follows. Denote the Logistic cdf with $\Lambda$ and its pdf with $\lambda$.

	First observe that
	$$Pr(Y_i = 1 | X_i, b_i) = Pr(\epsilon_i > -b_i x_i) = 1- Pr(\epsilon_i < - b_i x_i) = 1- \Lambda(-b_i x_i) = \Lambda(b_ix_i)$$
	Thus, assuming that $b_i$ and $\epsilon_i$ are independent:
	$$Pr(Y_i = 1|X_i) = \int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db$$

	The likelihood of this model for a single observation is $$L(Y_i,X_i;\mu,\sigma) = \left(\int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db\right)^{Y_i}\left(1 - \int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db\right)^{1-Y_i}$$

	Thus, for \textbf{fixed} random draws $\left\{u_i\right\}_{m=1}^{M}$ from $\mathcal{N}(0,1)$, the simulated likelihood is
	$$\hat L_M(Y_i,X_i,\mu,\sigma) 
	= 
	\left(\frac{1}{M}\sum_{m=1}^M\Lambda(\underbrace{(\mu+\sigma u_{im})}_{\equiv b}X_i)\right)^{Y_i}
	\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda(\underbrace{(\mu+\sigma u_{im})}_{\equiv b}X_i)\right)^{1-Y_i}
	$$
	
	Clearly, taking logs (of the product of all the likelihoods) allows us to recover the simulated log-likelihood of the full sample. 

	Let's differentiate with respect to $\mu$ and $\sigma$!

	For $\mu$:
	\begin{align*}
		\frac{\partial}{\partial \mu} & \frac{1}{N}\sum_{i}^{N} Y_i log\left(\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) + (1-Y_i) log\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) \\
		= & \frac{1}{N}\sum_i^N \left[\frac{Y_iX_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)}{\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} - \frac{(1-Y_i)X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)}{1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} \right] 
	\end{align*}
	For $\sigma$:
		\begin{align*}
		\frac{\partial}{\partial \sigma} & \frac{1}{N}\sum_{i}^{N} Y_i log\left(\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) + (1-Y_i) log\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) \\
		= & \frac{1}{N}\sum_i^N \left[\frac{Y_i X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)u_{im}}{\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} - \frac{(1-Y_i)X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i) u_{im}}{1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)}\right]
		\end{align*}
	\item See \textit{max\_sml.m} file.
	\item See \textit{max\_sml.m} file.
	\item These results are invariant to initial conditions:
	$$\begin{bmatrix}\mu \\ ln(\sigma)\end{bmatrix} 
	= \begin{bmatrix} 1.6011 \\ 0.5139 \end{bmatrix}$$
	\item Let $\hat \theta_{MSL}$ denote the estimated parameters from the method of simulated likelihood and the true parameters $\theta = [\mu, \sigma]'$. Under proper specification, we know that
	$$\sqrt{N}(\hat \theta_{MSL} - \theta) \to 
	\mathcal{N}(\mathbf{0},\mathbf{H}(\theta)^{-1})$$
	By the delta-method, with $g(\theta) \equiv [\mu ln(\sigma)]'$, we have that
	$$\sqrt{N}(g(\hat \theta_{MSL}) - g(\theta)) \to 
	\mathcal{N}(\mathbf{0},\mathbf{G}\mathbf{H}(\theta)^{-1}\mathbf{G})$$
	where $$\mathbf{G} \equiv \frac{\partial g(\theta)}{\partial \theta} = \begin{bmatrix}1 & 0 \\ 0 & \frac{1}{\sigma}\end{bmatrix}$$

	When we calculate the square root of the diagonal elements of $\frac{1}{N}\mathbf{H}(\hat \theta)^{-1}$, we derive the standard errors of our estimates of $\mu$ and $\ln(\sigma)$: $(0.2633,0.3884)$.
	
	\item To calculate the standard errors for the incorrect model, we replace $\mathbf{H}(\theta)^{-1}$ with $\mathbf{H}(\theta)^{-1}\mathbf{V_s}\mathbf{H}(\theta)^{-1}$, where $\mathbf{V_s}$ is the population variance-covariance matrix of the scores---defined by the the criterion function we are maximizing. 

	Again, we use the analogy principle to replace the asymptotic variance terms with the simulated likelihood, sample averages. This yields standard errors of $\mu$ and $ln(\sigma)$: $(0.4839,0.6417)$.

\end{enumerate}

\end{document}