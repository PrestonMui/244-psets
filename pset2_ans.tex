\documentclass[11pt]{article}
\usepackage{minibox}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{ bbold }
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#2 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\subsection*{2. Logit MLE}

\begin{enumerate}[a)]

	\item Derive the score of the logit-likelihood:
	First, noting that
	\begin{align*}
		\frac{\partial \Lambda(X_i'\beta)}{\partial \beta} &= \frac{\exp(X_i' \beta)X_i'(1 + X_i'\beta) - \exp(X_i'\beta) \exp(X_i'\beta) X_i'}{(1+\exp(X_i'\beta))^2} = \frac{\exp(X_i'\beta)X_i'}{(1 + \exp(X_i'\beta))^2}
	\end{align*}
	The score of the logit log-likelihood is therefore
	\begin{align*}
		s(\beta) &= \sum_i \frac{Y_i \exp(X_i'\beta)X_i' }{\Lambda(X_i'\beta)(1 + \exp(X_i'\beta))^2} -
			\frac{(1 - Y_i)\exp(X_i'\beta)X_i'}{(1 - \Lambda(X_i'\beta))(1 + \exp(X_i'\beta))^2} \\
		&= \sum_i \Big( Y_i - (1 - Y_i)\exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)}\\
		&= \sum_i \Big( Y_i(1 + \exp(X_i'\beta)) - \exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)} \\
		&= \sum_i \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} \Big) X_i'
	\end{align*}
	
	\item The moment condition identifying $\beta_{ML}$ is
	\begin{align*}
		E \bigg[ \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)}\Big) X_i' \bigg] &= 0 \\
		E \bigg[ E[Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} | X_i] X_i' \bigg] &= 0
	\end{align*}
	
	\item The moment conditions identifying $\beta_{NLLS}$ are
	\begin{align*}
		0 &= E \bigg[ -2 (Y_i - \Lambda(X_i'\beta)) \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
		&= E \bigg[ E[(Y - \Lambda(X_i'\beta)) | X_i] \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
	\end{align*}

	\item Under which conditions does $\beta_{NLLS}$ coincide with $\beta_{ML}$? The moment conditions will coincide when $E[Y_i - \Lambda(X_i'\beta) | X_i'] = 0$ for all $X_i$; that is, they coincide when the logit model is correctly specified.

\end{enumerate}


\subsection*{3. Matlab Probit DGP}

\begin{enumerate}[a)]

	\item (Matlab program attached)

	\item The ML point estimates of $\hat{\beta}^{con} = (\hat{\alpha},\hat{b})$ are $(-0.0175,0.9159)$. The standard errors are $(0.3030,0.3607)$, respectively.

	\item Score test: Following Wooldridge (2010), page 570, the $LM$ statistic is the ESS from the following regression
	\begin{align*}
		\frac{\hat{u}_i}{\sqrt{\hat{G}_i (1 - \hat{G}_i)}} &= \alpha \frac{\hat{g}_i}{\sqrt{\hat{G}_i(1 - \hat{G}_i)}} x_i + \gamma  \frac{\hat{g}_i}{\sqrt{\hat{G}_i(1 - \hat{G}_i)}} z_i
	\end{align*}
	where $x_i$ is the regressor matrix in the unconstrained regression (constant and $X$) and $z_i$ is the vector of $X_i^2$. The ESS from this regression was $0.2963$, which has a p-value of $0.58621$. So, one does not reject the null that $b_2 = 0$.

	\item The unrestricted model yields point estimates of $\hat{\beta}^{unc} = (\hat{\alpha},\hat{b},\hat{b_2}) = (-0.0435, 0.9175, 0.0428)$. The Wald test will test the null $g(\beta) = 0$ where
	\begin{align*}
		g(\beta) &\equiv (0, 0, 1) \cdot \beta = b_2 \\
		G(\beta) &\equiv \frac{\partial g(\beta)}{\partial \beta} = (0, 0, 1)
	\end{align*}
	and the Wald statistic is given by
	\begin{align*}
		N \cdot \hat{b_2} \cdot \bigg( G \cdot \frac{1}{\sqrt{N}}H^{-1} \cdot G' \bigg)^{-1} \cdot \hat{b_2}
	\end{align*}
	where $H$ is the average Hessian at the ML estimate. Because we know that the model is correctly specified, I use the inverse Hessian instead of the sandwich estimator. The Wald evaluates to 7.4592, which has a p-value of $0.0063$ under the $\chi^2$ distribution with d.f. 1, so one rejects the null that $b_2 = 0$. This is starkly different from the score test result, which did not reject the null. This makes sense, as the Wald tends to reject more than the LM test. If one bumps the number of observations up (say, to 5000), both tests reject the null.

\end{enumerate}

\subsection*{6. Random Coefficient Binary Choice Model:}
\begin{enumerate}[a)]
	\item As was derived in the lecture notes, the simulated log likelihood can be derived as follows. Denote the Logistic cdf with $\Lambda$ and its pdf with $\lambda$.

	First observe that
	$$Pr(Y_i = 1 | X_i, b_i) = Pr(\epsilon_i > -b_i x_i) = 1- Pr(\epsilon_i < - b_i x_i) = 1- \Lambda(-b_i x_i) = \Lambda(b_ix_i)$$
	Thus, assuming that $b_i$ and $\epsilon_i$ are independent:
	$$Pr(Y_i = 1|X_i) = \int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db$$

	The likelihood of this model for a single observation is $$L(Y_i,X_i;\mu,\sigma) = \left(\int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db\right)^{Y_i}\left(1 - \int \Lambda(b x_i) \frac{1}{\sigma}\phi(\frac{b_i - \mu}{\sigma})db\right)^{1-Y_i}$$

	Thus, for \textbf{fixed} random draws $\left\{u_i\right\}_{m=1}^{M}$ from $\mathcal{N}(0,1)$, the simulated likelihood is
	$$\hat L_M(Y_i,X_i,\mu,\sigma) 
	= 
	\left(\frac{1}{M}\sum_{m=1}^M\Lambda(\underbrace{(\mu+\sigma u_{im})}_{\equiv b}X_i)\right)^{Y_i}
	\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda(\underbrace{(\mu+\sigma u_{im})}_{\equiv b}X_i)\right)^{1-Y_i}
	$$
	
	Clearly, taking logs (of the product of all the likelihoods) allows us to recover the simulated log-likelihood of the full sample. 

	Let's differentiate with respect to $\mu$ and $\sigma$!

	For $\mu$:
	\begin{align*}
		\frac{\partial}{\partial \mu} & \frac{1}{N}\sum_{i}^{N} Y_i log\left(\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) + (1-Y_i) log\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) \\
		= & \frac{1}{N}\sum_i^N \left[\frac{Y_iX_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)}{\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} - \frac{(1-Y_i)X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)}{1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} \right] 
	\end{align*}
	For $\sigma$:
		\begin{align*}
		\frac{\partial}{\partial \sigma} & \frac{1}{N}\sum_{i}^{N} Y_i log\left(\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) + (1-Y_i) log\left(1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)\right) \\
		= & \frac{1}{N}\sum_i^N \left[\frac{Y_i X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i)u_{im}}{\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)} - \frac{(1-Y_i)X_i\frac{1}{M}\sum_{m=1}^M\lambda((\mu+\sigma u_{im})X_i) u_{im}}{1-\frac{1}{M}\sum_{m=1}^M\Lambda((\mu+\sigma u_{im})X_i)}\right]
		\end{align*}
	\item See \textit{max\_sml.m} file.
	\item See \textit{max\_sml.m} file.
	\item These results are invariant to initial conditions:
	$$\begin{bmatrix}\mu \\ ln(\sigma)\end{bmatrix} 
	= \begin{bmatrix} 1.6011 \\ 0.5139 \end{bmatrix}$$
	\item Let $\hat \theta_{MSL}$ denote the estimated parameters from the method of simulated likelihood and the true parameters $\theta = [\mu, \sigma]'$. Under proper specification, we know that
	$$\sqrt{N}(\hat \theta_{MSL} - \theta) \to 
	\mathcal{N}(\mathbf{0},\mathbf{H}(\theta)^{-1})$$
	By the delta-method, with $g(\theta) \equiv [\mu ln(\sigma)]'$, we have that
	$$\sqrt{N}(g(\hat \theta_{MSL}) - g(\theta)) \to 
	\mathcal{N}(\mathbf{0},\mathbf{G}\mathbf{H}(\theta)^{-1}\mathbf{G})$$
	where $$\mathbf{G} \equiv \frac{\partial g(\theta)}{\partial \theta} = \begin{bmatrix}1 & 0 \\ 0 & \frac{1}{\sigma}\end{bmatrix}$$

	When we calculate the square root of the diagonal elements of $\frac{1}{N}\mathbf{H}(\hat \theta)^{-1}$, we derive the standard errors of our estimates of $\mu$ and $\ln(\sigma)$: $(0.2633,0.3884)$.
	
	\item To calculate the standard errors for the incorrect model, we replace $\mathbf{H}(\theta)^{-1}$ with $\mathbf{H}(\theta)^{-1}\mathbf{V_s}\mathbf{H}(\theta)^{-1}$, where $\mathbf{V_s}$ is the population variance-covariance matrix of the scores---defined by the the criterion function we are maximizing. 

	Again, we use the analogy principle to replace the asymptotic variance terms with the simulated likelihood, sample averages. This yields standard errors of $\mu$ and $ln(\sigma)$: $(0.4839,0.6417)$.

\end{enumerate}

\end{document}