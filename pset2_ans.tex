\documentclass[11pt]{article}
\usepackage{minibox}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{ bbold }
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#2 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\subsection*{2. Logit MLE}

\begin{enumerate}[a)]

	\item Derive the score of the logit-likelihood:
	First, noting that
	\begin{align*}
		\frac{\partial \Lambda(X_i'\beta)}{\partial \beta} &= \frac{\exp(X_i' \beta)X_i'(1 + X_i'\beta) - \exp(X_i'\beta) \exp(X_i'\beta) X_i'}{(1+\exp(X_i'\beta))^2} = \frac{\exp(X_i'\beta)X_i'}{(1 + \exp(X_i'\beta))^2}
	\end{align*}
	The score of the logit log-likelihood is therefore
	\begin{align*}
		s(\beta) &= \sum_i \frac{Y_i \exp(X_i'\beta)X_i' }{\Lambda(X_i'\beta)(1 + \exp(X_i'\beta))^2} -
			\frac{(1 - Y_i)\exp(X_i'\beta)X_i'}{(1 - \Lambda(X_i'\beta))(1 + \exp(X_i'\beta))^2} \\
		&= \sum_i \Big( Y_i - (1 - Y_i)\exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)}\\
		&= \sum_i \Big( Y_i(1 + \exp(X_i'\beta)) - \exp(X_i'\beta) \Big) \frac{X_i'}{1 + \exp(X_i'\beta)} \\
		&= \sum_i \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} \Big) X_i'
	\end{align*}
	
	\item The moment condition identifying $\beta_{ML}$ is
	\begin{align*}
		E \bigg[ \Big( Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)}\Big) X_i' \bigg] &= 0 \\
		E \bigg[ E[Y_i - \frac{\exp(X_i'\beta)}{1 + \exp(X_i'\beta)} | X_i] X_i' \bigg] &= 0
	\end{align*}
	
	\item The moment conditions identifying $\beta_{NLLS}$ are
	\begin{align*}
		0 &= E \bigg[ -2 (Y_i - \Lambda(X_i'\beta)) \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
		&= E \bigg[ E[(Y - \Lambda(X_i'\beta)) | X_i] \frac{\exp(X_i'\beta)}{(1 + \exp(X_i'\beta))^2} X_i' \bigg] \\
	\end{align*}

	\item Under which conditions does $\beta_{NLLS}$ coincide with $\beta_{ML}$? The moment conditions will coincide when $E[Y_i - \Lambda(X_i'\beta) | X_i'] = 0$ for all $X_i$; that is, they coincide when the logit model is correctly specified.

\end{enumerate}


\subsection*{3. Matlab Probit DGP}

\begin{enumerate}[a)]

	\item (Matlab program attached)

	\item The ML point estimates of $\hat{\beta}^{con} = (\hat{\alpha},\hat{b})$ are $(-0.0175,0.9159)$. The standard errors are $(0.0918,0.1301)$.

	\item Score test: Regressing the restricted model's generalized residuals, given by
	\begin{align*}
		gres_i &= \frac{Y_i \phi(X_i'\hat{\beta}^{con})}{\Phi(X_i'\hat{\beta}^{con})} - \frac{(1 - Y_i)\phi(X_i' \hat{\beta}^{con})}{1 - \Phi(X_i'\hat{\beta}^{con})}
	\end{align*}
	on $X_i^2$ yields a coefficient of $0.0039$ with a p-value of effectively 0, rejecting the null of 0.

	\item The unrestricted model yields point estimates of $\hat{\beta}^{unc} = (\hat{\alpha},\hat{b},\hat{b_2}) = (-0.0435, 0.9175, 0.0428)$. The wald test, which tests $g(\beta) = 0$, where
	\begin{align*}
		g(\beta) &\equiv (0, 0, 1) \cdot \beta = b_2 \\
		G(\beta) &\equiv \frac{\partial g(\beta)}{\partial \beta} = (0, 0, 1)
	\end{align*}
	and the Wald statistic is given by
	\begin{align*}
		N \cdot \hat{b_2} \cdot \bigg( G \cdot H^{-1} \cdot G' \bigg)^{-1} \cdot \hat{b_2}
	\end{align*}
	where $H$ is the average Hessian at the ML estimate. Here, the Wald evaluates to 0.3336, which has a p-value of $0.5636$ under the $\chi^2$ distribution with d.f. 1, much higher than the $0$ obtained in part b.

\end{enumerate}

\end{document}