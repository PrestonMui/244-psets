\documentclass[11pt]{article}
\usepackage[top=1in, bottom=1.25in, left=1.25in, right=1.25in]{geometry}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage[parfill]{parskip}
\usepackage{enumerate,amsmath,amsthm,amssymb,bbold}
\usepackage{minibox,graphicx,caption,booktabs,pdflscape,multirow,verbatim,subcaption,pdfpages,longtable}
\usepackage[shortlabels]{enumitem}
\usepackage[T1]{fontenc}
\usepackage[section]{placeins} %to keep graphs and tables in the very section to which they belong 
%-----------------------------------------------------------------------------
\begin{document}
\begin{center}
\framebox[\linewidth]{ 
	\minibox[c]{
	\Large Homework \#3 \\ \\
	Professor: Pat Kline \\ \\
	Students: Christina Brown, Sam Leone, Peter McCrory, Preston Mui
	}
}
\end{center}

\bigskip \textbf{Reweighting}

\bigskip \textbf{Matching}

\bigskip \textbf{2SLS}

\begin{enumerate}[(a)]

	\item The moment conditions defining the 2SLS estimator are
	\begin{align*}
		E \left[ Z_i \epsilon_i \right] &= 0 \\
		E \left[ Z_i^2 \epsilon_i \right] &= 0
	\end{align*}
	they imply each other, since functions of uncorrelated variables are uncorrelated with each other.

	\item The moment conditions defining the control function estimator are
	\begin{align*}
		E \left[ Z_i \epsilon_i \right] &= 0 \\
		E \left[ Z_i v_i \right] &= 0 \\
		E \left[ \epsilon_i | v_i \right] &= \rho \frac{\sigma_{\epsilon}}{\sigma_v} v_i \\
		E \left[ (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2 - \rho \frac{\sigma_{\epsilon}}{\sigma_v} v_i)X_i \right] &= 0 \\
		E \left[ (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2 - \rho \frac{\sigma_{\epsilon}}{\sigma_v} v_i)X_i^2 \right] &= 0 \\
		E \left[ (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2 - \rho \frac{\sigma_{\epsilon}}{\sigma_v} v_i)v_i \right] &= 0
	\end{align*}

	\item The control function estimator relies on stronger conditions than the 2SLS estimator. The only condition in the 2SLS estimator is that the instrument, $Z$, is uncorrelated with the error term $\epsilon$. However, the control function estimator relies on specifying the exogenous relationship between $X$ and $Z$, in particular that the conditional expection of $X$ given $Z$ is linear.

	\item The control function would work better when $Z$ is weak, because it brings to bear more moment restrictions than the 2SLS estimator, whereas the 2SLS estimator only relies on the exogeneity of $Z$. %Should add more but not sure what

	\item

	The table below contains the estimates for $\beta_1$ and $\beta_2$ under the 2SLS and control function methods.

	\begin{enumerate}[(i)]
	 
		\item Overall, for all values of $\gamma$, the control function estimate appears to be more efficient (the spread of the estimates is much lower), because the control function's ``first-stage'' is correctly specified, and the estimation involves more moment restrictions.
		
		\item As $\gamma_1$ shrinks, the mean 2sls estimate falls (away from the true value of $1.0$), although the median appears to remain unbiased. The interquartile range of both estimators increases, although the IQR for the 2sls estimate increases much faster than that of the control function. The control function estimates appear to remain both median and mean-unbiased.

		\item If one wanted to test the hypothesis that $\rho \frac{\sigma_{\epsilon}}{\sigma_{v}} = 0$, one would need an additional term in the variance of the coefficient on $v_i$. In particular, by using the two-stage estimate formulas for the asymptotic variance of the estimate of $\rho \frac{\sigma_{\epsilon}}{\sigma_{v}}$, one would see that in the ``sandwich'' estimator, one would replace the term
		\begin{align*}
			E[s_i(\theta_0, \gamma^*)s_i(\theta_0, \gamma^*)']
		\end{align*}
		where $\theta_0 = (\beta_0, \beta_1, \beta_2, \rho \frac{\sigma_{\epsilon}}{\sigma_{v}})$ and $\gamma^* = (\gamma_0, \gamma_1)$, and $s_i$ being the score function (gradient of the squared residual with respect to $\theta_0$), with this term
		\begin{align*}
			& E[g_i(\theta_0, \gamma^*)g_i(\theta_0, \gamma^*)'] \\
			\mbox{s. t. } g_i(\theta_0, \gamma^*) &\equiv s_i(\theta_0, \gamma^*) + E \left[ \nabla_\gamma s_i(\theta_0, \gamma^*) \right]  r_i (\theta_0, \gamma^*)
		\end{align*}
		where $r_i$ is the asymptotic variance of $\sqrt{N}(\hat{\gamma} - \gamma^*)$ and can be obtained from the ``first-stage'' regression. To construct an estimate of the standard error, one can simply plug in the population analogues into this sandwich estimator and run a t-test with the modified standard error.

	\end{enumerate}

	\centering
	\input{pset3_q3_table}

\end{enumerate}

\bigskip \textbf{Bootstrap OLS}

\bigskip \textbf{Bootstrap Probit}

\end{document}